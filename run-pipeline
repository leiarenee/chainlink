#!/bin/bash
set -e
trap 'catch $? $LINENO' ERR

function catch() {
  echo "Bash Script Error $1 occurred on Line $2"
  bash_error=true
  if [[ $UPLOAD_WORKFOLDER == "always" ]] || [[ $UPLOAD_WORKFOLDER == "onerror" && $tg_err -ne 0 ]]
  then
    set +e
    upload_temp_folder
    set -e
  fi
}

function import_env_vars(){
  env_files=(".env" "override.env")
  IFS=$'\n'

  # Declare environment varibles from env files
  for env_file in ${env_files[@]}
  do
    if [ -f $env_file ]
    then
      for env_var in $(cat $env_file)
      do
        first_char=${env_var:0:1}
        if [[ ! $first_char == "#" ]]
        then
          export $env_var
          
        fi
      done
    fi
  done

  IFS=$' '
}

function job_definitions(){
  script_dir=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
  tg_command=${1:-$TG_COMMAND}
  cron_job=$2

  if [ -z $AWS_BATCH_JOB_ID ] # For local dev environment
  then
    uuid=$(uuidgen)
    export SF_EXECUTION_NAME=Dev-$(whoami)-$uuid
    export AWS_BATCH_JOB_ID=Process-$$
    import_env_vars
  fi

  [ -z $SQS_MESSAGE_GROUP_ID ] && export SQS_MESSAGE_GROUP_ID=$AWS_BATCH_JOB_ID

  job_name=$WORKSPACE_ID/$SF_EXECUTION_NAME/$AWS_BATCH_JOB_ID
}

function fetch_from_gitlab(){
  echo
  echo Fetching repository from gitlab
  echo

  # Download repo from gitlab api, project version can be version tag (ex v0.1.1) or sha commit or branch name
  curl --header "PRIVATE-TOKEN: $token" \
    "https://gitlab.com/api/v4/projects/$GITLAB_PROJECT_ID/repository/archive.zip?sha=$GITLAB_PROJECT_VERSION" \
    -o downloaded_repo.zip

  # Unzip the zipped file
  unzip -o downloaded_repo.zip
  rm downloaded_repo.zip

}

function fetch_from_github(){
  echo
  echo Fetching repository from Github, Ref:$REPO_REFERENCE
  echo

  #repo_download_link=https://api.github.com/repos/${REPO_ACCOUNT}/${REPO_NAME}/tarball/${REPO_REFERENCE}
  # redirects here
  repo_download_link=https://codeload.github.com/${REPO_ACCOUNT}/${REPO_NAME}/legacy.tar.gz/refs/heads/${REPO_REFERENCE}

  [[ $REPO_TYPE == "private" ]] && token_header=--header\="Authorization:token $token" 

  wget $token_header --header=Accept:application/vnd.github.v3.raw -O - $repo_download_link | tar xz

}

# Fetch Repo
function fetch_repository(){

  # Get secrets from envvars
  user=$(echo $REPO_ACCESS_TOKEN | jq -r .user)
  token=$(echo $REPO_ACCESS_TOKEN | jq -r .token)

  case $VCS_PROVIDER in

    gitlab )
      fetch_from_gitlab
    ;;

    github )
      fetch_from_github
    ;;

    *)
      echo "Unknown REPO_TYPE, Use One of GITHUB/GITLAB"
      exit -1
    ;;

  esac

  # Rename extracted folder name to repo name

  dir_name=$(ls -d $REPO_ACCOUNT-$REPO_NAME*)
  echo Downloaded directory name : $dir_name
  commit=$(echo $dir_name | sed s/$REPO_ACCOUNT-$REPO_NAME-//g)
  echo Commit Hash : $commit
  export COMMIT_HASH=$commit
  cp -R $dir_name $WORKSPACE_ID
  echo
}

function copy_local_app_repo(){

  mkdir temp-job/$LOCAL_APP_REPO
  mkdir temp-job/$WORKSPACE_ID
  rsync -a --exclude-from=../$LOCAL_APP_REPO/.gitignore ../$LOCAL_APP_REPO/ temp-job/$LOCAL_APP_REPO
  rsync -a temp-job/$LOCAL_APP_REPO/ temp-job/$WORKSPACE_ID

  
}

function fetch_app_repo(){
  [ -d temp-job ] && rm -rf temp-job
  mkdir temp-job
  
  if [[ $USE_LOCAL_REPO == "true" ]]
  then 
    copy_local_app_repo
  else
    cd temp-job
    fetch_repository
    cd ..
  fi
  
}

# Upload Temp Folder
function upload_temp_folder(){
  # UPLOAD_WORKFOLDER never|always|onerror
  # onerror will run on Terragrunt errors and bash errors
  # never disables it - use for local dev env
  # always runs it on every execution

  if [[ $UPLOAD_WORKFOLDER == "always" ]] || [[ $UPLOAD_WORKFOLDER == "onerror" && $tg_err -ne 0 ]] || [[ ! -z $bash_error && $UPLOAD_WORKFOLDER != "never" ]]
  then
    echo Uploading temp folder
    # Find Dublicates and replace with symlinks
    # Thanks to https://github.com/pauldreik/rdfind
    cd $script_dir
    rdfind -makesymlinks true temp-job
    # Compress Work folder
    tar -czf temp-job.tar.gz temp-job
    # Upload to S3
    aws configure set default.s3.max_concurrent_requests 10
    aws configure set default.s3.multipart_threshold 1000MB # Disable concurrency since it doesn't work in docker
    aws configure set default.s3.multipart_chunksize 100MB
    s3_file_name="$job_name/temp-job/$AWS_BATCH_JOB_ID.tar.gz"
    #aws --profile $PIPELINE_AWS_PROFILE s3 cp temp-job.tar.gz s3://$S3_JOBS_BUCKET/$s3_file_name
    aws --profile $PIPELINE_AWS_PROFILE s3api put-object --bucket $S3_JOBS_BUCKET --key $s3_file_name --body temp-job.tar.gz
    rm temp-job.tar.gz
  fi
}

# Fetch Secrets from secret manager
function fetch_secret {
  echo -e "${GREEN}- Feching Secret $1 from secret manager ${NC}";echo
  secret_value=$(aws secretsmanager get-secret-value --secret-id $2)
  echo "Successfully fetched $1";
  export $1=$(echo $secret_value | jq .SecretString | sed s/[\\]//g  | sed s/^\"//g | sed s/\}\"/\}/g )
}

# Configure aws profile
function configure_aws_profile(){
  pipeline_aws_access_key_id=$(echo $PIPELINE_AWS_ACCESS | jq -r .aws_access_key_id)
  pipeline_aws_secret_access_key=$(echo $PIPELINE_AWS_ACCESS | jq -r .aws_secret_access_key)
  
  # Default
  if [ ! $(cat $HOME/.aws/credentials | grep default) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[default]
aws_access_key_id = $pipeline_aws_access_key_id
aws_secret_access_key = $pipeline_aws_secret_access_key
region = $PIPELINE_AWS_REGION
EOF
  fi

  # Infra Structure account
  if [ ! $(cat $HOME/.aws/credentials | grep $PIPELINE_AWS_PROFILE) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[$PIPELINE_AWS_PROFILE]
aws_access_key_id = $pipeline_aws_access_key_id
aws_secret_access_key = $pipeline_aws_secret_access_key
region = $PIPELINE_AWS_REGION
EOF
  fi

  # Target account
  aws_access_key_id=$(echo $TARGET_AWS_ACCESS | jq -r .aws_access_key_id)
  aws_secret_access_key=$(echo $TARGET_AWS_ACCESS | jq -r .aws_secret_access_key)
  if [ ! $(cat $HOME/.aws/credentials | grep $TARGET_AWS_PROFILE) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[$TARGET_AWS_PROFILE]
aws_access_key_id = $aws_access_key_id
aws_secret_access_key = $aws_secret_access_key
region = $TARGET_AWS_REGION
EOF
  fi
}


function find_modules(){
  echo Getting Modules List...
  #tg_modules=$(python3 -m runtask json-modules)
  tg_groups=$(python3 -m runtask json-groups)
  export TG_MODULES_LIST=$(echo $tg_groups | jq 'values[][]' | jq . --slurp -c)
  echo "Terragrunt will run the modules in the following order."
  echo $tg_groups | jq .
  export TG_MODULES_COMPLETED=[]
  export TG_MODULES_COUNT=$(echo $TG_MODULES_LIST | jq length)
  echo
  echo Total number of modules to be processed: $TG_MODULES_COUNT
  echo
  sleep 1
  
}

function test_progress(){
  export TG_PARRENT_DIR=temp-job
  len=$(($TG_MODULES_COUNT - 1))
  for i in $(seq 0 0)
    do
      source terragrunt/scripts/job_complete.sh $STACK_FOLDER/$(echo $TG_MODULES_LIST | jq -r --arg i $i '.[$i|tonumber]')
    done
}

# Run Terragrunt
function run_terragrunt(){

  # Prepare command line arguments

  if [[ $RUN_ALL == "true" ]]
  then
    tg_run_all=run-all
    tg_non_interactive=--terragrunt-non-interactive
  fi

  working_dir=temp-job/$WORKSPACE_ID/$STACK_FOLDER/$RUN_MODULE

  # Escape / Character
  escaped_stack_path=$(echo "$(pwd)/temp-job/" | sed s/\\//\\\\\\//g)
  
  # Prepare command line arguments for interactive or non-interactive usage
  if [[ ! $INTERACTIVE == "true" ]]
  then
    export TG_DISABLE_CONFIRM=true
    stderr_output=/dev/null
    log_level=debug
    no_color=-no-color
  else
    stderr_output=1
    log_level=warn
  fi
  set +e
  # Run terragrunt
  terragrunt $tg_run_all $tg_command \
    --terragrunt-debug --terragrunt-working-dir $working_dir  \
    $tg_non_interactive --terragrunt-log-level $log_level $no_color \
    > >(tee stdout.log) \
    2> >(tee stderr.log >&$stderr_output) 

  # Get Exit Code
  export tg_err=$?
  [ $tg_err -ne 0 ] && echo Terragrunt Exitcode: $tg_err

  # Prepare a consise error log
  cat stderr.log | grep -vE 'locals|msg=run_cmd|msg=Detected|msg=Included|msg=\[Partial\]|msg=Executing|msg=WARN|msg=Setting|^$|msg=Generated|msg=Downloading|msg=The|msg=Are|msg=Reading|msg=Copying|msg=Debug|must wait|msg=Variables|msg=Dependency|msg=[0-9] error occurred|Cannot process|=>|msg=Stack|msg=Unable|errors occurred|sensitive|BEGIN RSA|\[0m|You may now|any changes|should now|If you ever|If you forget|- Reusing previous| \* exit status|Include this file|Terraform can guarantee|^\t\* |^ prefix' \
    | sed s/$escaped_stack_path//g > stderr_filtered.log
  
  # If error occurs write filtered error into terminal (stdout)
  [ $tg_err -ne 0 ] && cat stderr_filtered.log | grep -E 'error|Error'

  set -e

}

# Put outputs to S3
function put_outputs_to_s3(){
  if [[ $tg_command == "apply" ]] && [[ $RUN_ALL == "true" ]] && [ $tg_err -eq 0 ] 
  then
    s3_parent_key="$S3_JOBS_BUCKET/$job_name"
    cd $script_dir/temp-job/$WORKSPACE_ID/$STACK_FOLDER/job-resources
    tg_disable_confirm=$TG_DISABLE_CONFIRM
    export TG_DISABLE_CONFIRM=true
    terragrunt output job_resources > outputs.txt
    export TG_DISABLE_CONFIRM=$tg_disable_confirm

    if [[ $UPLOAD_JOB_RESOURCES == "true" ]]
    then
      echo "Outputs for Job $job_name as text"
      cat outputs.txt
      aws --profile $PIPELINE_AWS_PROFILE s3 cp outputs.txt s3://$s3_parent_key/job-resources/outputs.txt
    fi

    echo "Outputs for Job $job_name as json"
    cat outputs.txt | jq -r '.|fromjson' > outputs.json
    cat outputs.json | jq .

    if [[ $UPLOAD_JOB_RESOURCES == "true" ]]
    then
      aws --profile $PIPELINE_AWS_PROFILE s3 cp outputs.json s3://$s3_parent_key/job-resources/outputs.json
    fi
  fi
}

function delete_temp_files(){
  rm -f *.log
}

function createSqsQueue(){ 
  if [[ $SEND_SQS_MESSAGES == "true" ]]
  then

    if [ -z $cron_job ]
    then
      sqs_queue_name=$SF_EXECUTION_NAME
      message_retention_period=600
    else
      sqs_queue_name=cron-job
      message_retention_period=600000
    fi

    if [ -z $SQS_QUEUE_URL ] || [[ $SQS_QUEUE_URL == "_" ]]
    then
      export SQS_QUEUE_URL=$(aws --profile $SQS_AWS_PROFILE sqs create-queue --queue-name $sqs_queue_name.fifo --attributes "{\"FifoQueue\": \"true\", \"ContentBasedDeduplication\": \"true\",\"MessageRetentionPeriod\":\"$message_retention_period\"}" | jq -r .QueueUrl)
    fi
    
    if [ -z "$SQS_QUEUE_URL" ]
    then
      echo "SQS_QUEUE_URL could not be evaluted."
      exit 1
    fi

    echo "SQS Messaging endpoind for the Job is $SQS_QUEUE_URL"
  fi
}

function send_sqs_status(){
  # send job start sqs message
  if [[ $SEND_SQS_MESSAGES == "true" ]]
  then
    aws --profile $SQS_AWS_PROFILE sqs send-message --queue-url "$SQS_QUEUE_URL" --message-group-id "$SQS_MESSAGE_GROUP_ID" --message-body "$1"
  fi
}

function remove_event_rule(){
  set +e
  if [[ $tg_command == 'destroy' ]]
  then
    python3 -m runtask delete-event --name $EVENTBRIDGE_RULE
  fi
  set -e
}

function send_progress(){
 send_sqs_status "{\"message\":{\"status\":\"Job Started\",\"progress\":$@,\"jobId\":\"$AWS_BATCH_JOB_ID\"}}"
}

# Main Routine
echo "Running pipeline with Entrypoint arguments $@";echo 
job_definitions $@
echo $job_name Job Started
fetch_app_repo
configure_aws_profile
createSqsQueue
send_progress $INITIAL_PROGRESS
remove_event_rule
delete_temp_files
find_modules
run_terragrunt
put_outputs_to_s3
send_progress $FINAL_PROGRESS
upload_temp_folder
echo "End of job routine for $job_name";echo

