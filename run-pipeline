#!/bin/bash
set -e
trap 'catch $? $LINENO' ERR



function catch() {
  echo "Bash Script Error $1 occurred on Line $2"
  bash_error=true
  if [[ $UPLOAD_WORKFOLDER == "always" ]] || [[ $UPLOAD_WORKFOLDER == "onerror" && $tg_err -ne 0 ]]
  then
    set +e
    upload_temp_folder
    set -e
  fi
}

function import_env_vars(){
  env_files=(".env" "override.env")
  IFS=$'\n'

  # Declare environment varibles from env files
  for env_file in ${env_files[@]}
  do
    if [ -f $env_file ]
    then
      for env_var in $(cat $env_file)
      do
        first_char=${env_var:0:1}
        if [[ ! $first_char == "#" ]]
        then
          export $env_var
        fi
      done
    fi
  done

  IFS=$' '
}

function job_definitions(){
  script_dir=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
  tg_command=${1:-$TG_COMMAND}
  cron_job=$2

  if [ -z $AWS_BATCH_JOB_ID ] # For local dev environment
  then
    uuid=$(uuidgen)
    export SF_EXECUTION_NAME=Dev-$(whoami)-$uuid
    export AWS_BATCH_JOB_ID=Process-$$
    import_env_vars
  fi

  [ -z $SQS_MESSAGE_GROUP_ID ] && export SQS_MESSAGE_GROUP_ID=$AWS_BATCH_JOB_ID

  job_name=$WORKSPACE_ID/$SF_EXECUTION_NAME/$AWS_BATCH_JOB_ID
}

# Upload Temp Folder
function upload_temp_folder(){
  # UPLOAD_WORKFOLDER never|always|onerror
  # onerror will run on Terragrunt errors and bash errors
  # never disables it - use for local dev env
  # always runs it on every execution

  if [[ $UPLOAD_WORKFOLDER == "always" ]] || [[ $UPLOAD_WORKFOLDER == "onerror" && $tg_err -ne 0 ]] || [[ ! -z $bash_error && $UPLOAD_WORKFOLDER != "never" ]]
  then
    echo Uploading temp folder
    # Find Dublicates and replace with symlinks
    # Thanks to https://github.com/pauldreik/rdfind
    cd $script_dir
    rdfind -makesymlinks true temp-job
    # Compress Work folder
    tar -czf temp-job.tar.gz temp-job
    # Upload to S3
    aws configure set default.s3.max_concurrent_requests 10
    aws configure set default.s3.multipart_threshold 1000MB # Disable concurrency since it doesn't work in docker
    aws configure set default.s3.multipart_chunksize 100MB
    s3_file_name="$job_name/temp-job/$AWS_BATCH_JOB_ID.tar.gz"
    #aws --profile $PIPELINE_AWS_PROFILE s3 cp temp-job.tar.gz s3://$S3_JOBS_BUCKET/$s3_file_name
    aws --profile $PIPELINE_AWS_PROFILE s3api put-object --bucket $S3_JOBS_BUCKET --key $s3_file_name --body temp-job.tar.gz
    rm temp-job.tar.gz
  fi
}

# Configure aws profile
function configure_aws_profile(){
  pipeline_aws_access_key_id=$(echo $PIPELINE_AWS_ACCESS | jq -r .aws_access_key_id)
  pipeline_aws_secret_access_key=$(echo $PIPELINE_AWS_ACCESS | jq -r .aws_secret_access_key)
  
  # Default
  if [ ! $(cat $HOME/.aws/credentials | grep default) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[default]
aws_access_key_id = $pipeline_aws_access_key_id
aws_secret_access_key = $pipeline_aws_secret_access_key
region = eu-west-1
EOF
  fi

  # Infra Structure account
  if [ ! $(cat $HOME/.aws/credentials | grep $PIPELINE_AWS_PROFILE) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[$PIPELINE_AWS_PROFILE]
aws_access_key_id = $pipeline_aws_access_key_id
aws_secret_access_key = $pipeline_aws_secret_access_key
region = eu-west-1
EOF
  fi


  # Target account
  aws_access_key_id=$(echo $TARGET_AWS_ACCESS | jq -r .aws_access_key_id)
  aws_secret_access_key=$(echo $TARGET_AWS_ACCESS | jq -r .aws_secret_access_key)
  if [ ! $(cat $HOME/.aws/credentials | grep $TARGET_AWS_PROFILE) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[$TARGET_AWS_PROFILE]
aws_access_key_id = $aws_access_key_id
aws_secret_access_key = $aws_secret_access_key
region = eu-west-1
EOF
  fi
}


function find_modules(){
  echo Modules List
  export TG_MODULES_LIST=$(python3 -m runtask json-modules)
  #export TG_MODULES_LIST=$(cat src/json/modules_list.json | jq .modules)
  echo $TG_MODULES_LIST | jq .
  export TG_MODULES_COMPLETED=[]
  export TG_MODULES_COUNT=$(echo $TG_MODULES_LIST | jq length)
  echo Total number of modules to be processed: $TG_MODULES_COUNT
  
}

function test_progress(){
  export TG_PARRENT_DIR=temp-job
  len=$(($TG_MODULES_COUNT - 1))
  for i in $(seq 0 0)
    do
      source terragrunt/scripts/job_complete.sh $STACK_FOLDER/$(echo $TG_MODULES_LIST | jq -r --arg i $i '.[$i|tonumber]')
    done
}

# Run Terragrunt
function run_terragrunt(){

  # Prepare command line arguments
  if [[ ! $INTERACTIVE == "true" ]]
  then 
    tg_non_interactive=--terragrunt-non-interactive
  fi

  if [[ $RUN_ALL == "true" ]]
  then
    tg_run_all=run-all
  fi

  working_dir=temp-job/$WORKSPACE_ID/$STACK_FOLDER/$RUN_MODULE

  # Escape / Character
  escaped_stack_path=$(echo "$(pwd)/temp-job/" | sed s/\\//\\\\\\//g)
  
  # Prepare command line arguments for interactive or non-interactive usage
  if [[ $tg_non_interactive ]]
  then
    export TG_DISABLE_CONFIRM=true
    stderr_output=/dev/null
    log_level=debug
    no_color=-no-color
  else
    stderr_output=1
    log_level=info
  fi
  set +e
  # Run terragrunt
  terragrunt $tg_run_all $tg_command \
    --terragrunt-debug --terragrunt-working-dir $working_dir  \
    $tg_non_interactive --terragrunt-log-level $log_level $no_color \
    > >(tee stdout.log) \
    2> >(tee stderr.log >&$stderr_output)

  # Get Exit Code
  tg_err=$?
  echo $tg_err
  # Prepare a consise error log
  cat stderr.log | grep -vE 'locals|msg=run_cmd|msg=Detected|msg=Included|msg=\[Partial\]|msg=Executing|msg=WARN|msg=Setting|^$|msg=Generated|msg=Downloading|msg=The|msg=Are|msg=Reading|msg=Copying|msg=Debug|must wait|msg=Variables|msg=Dependency|msg=[0-9] error occurred|Cannot process|=>|msg=Stack|msg=Unable|errors occurred|sensitive|BEGIN RSA|\[0m|You may now|any changes|should now|If you ever|If you forget|- Reusing previous| \* exit status|Include this file|Terraform can guarantee|^\t\* |^ prefix' \
    | sed s/$escaped_stack_path//g > stderr_filtered.log
  
  # If error occurs write filtered error into terminal (stdout)
  [ $tg_err -ne 0 ] && cat stderr_filtered.log | grep -E 'error|Error'

  set -e

}

# Put outputs to S3
function put_outputs_to_s3(){
  if [[ $tg_command == "apply" ]] && [[ $RUN_ALL == "true" ]] && [[ $UPLOAD_JOB_RESOURCES == "true" ]]
  then
    s3_parent_key="$S3_JOBS_BUCKET/$job_name"
    cd $script_dir/temp-job/$WORKSPACE_ID/$STACK_FOLDER/job-resources
    terragrunt output job_resources > outputs.txt
    echo "Outputs for Job $job_name as text"
    cat outputs.txt
    aws --profile $PIPELINE_AWS_PROFILE s3 cp outputs.txt s3://$s3_parent_key/job-resources/outputs.txt
    
    echo "Outputs for Job $job_name as json"
    cat outputs.txt | jq -r '.|fromjson' > outputs.json
    cat outputs.json
    aws --profile $PIPELINE_AWS_PROFILE s3 cp outputs.json s3://$s3_parent_key/job-resources/outputs.json
  fi
}

function delete_temp_files(){
  rm -f *.log
}

function createSqsQueue(){ 
  if [[ $SEND_SQS_MESSAGES == "true" ]]
  then

    if [ -z $cron_job ]
    then
      sqs_queue_name=$SF_EXECUTION_NAME
      message_retention_period=600
    else
      sqs_queue_name=cron-job
      message_retention_period=600000
    fi

    if [ -z $SQS_QUEUE_URL ] || [[ $SQS_QUEUE_URL == "_" ]]
    then
      export SQS_QUEUE_URL=$(aws --profile $SQS_AWS_PROFILE sqs create-queue --queue-name $sqs_queue_name.fifo --attributes "{\"FifoQueue\": \"true\", \"ContentBasedDeduplication\": \"true\",\"MessageRetentionPeriod\":\"$message_retention_period\"}" | jq -r .QueueUrl)
    fi
    
    if [ -z "$SQS_QUEUE_URL" ]
    then
      echo "SQS_QUEUE_URL could not be evaluted."
      exit 1
    fi

    echo "SQS Messaging endpoind for the Job is $SQS_QUEUE_URL"
  fi
}

function send_sqs_status(){
  # send job start sqs message
  if [[ $SEND_SQS_MESSAGES == "true" ]]
  then
    aws --profile $SQS_AWS_PROFILE sqs send-message --queue-url "$SQS_QUEUE_URL" --message-group-id "$SQS_MESSAGE_GROUP_ID" --message-body "$1"
  fi
}

function remove_event_rule(){
  set +e
  if [[ $tg_command == 'destroy' ]]
  then
    python3 -m runtask delete-event --name $EVENTBRIDGE_RULE
  fi
  set -e
}

# Main Routine
job_definitions $@
echo $job_name Job Started
configure_aws_profile
createSqsQueue
send_sqs_status "{\"message\":{\"status\":\"Job Started\",\"progress\":$INITIAL_PROGRESS,\"jobId\":\"$AWS_BATCH_JOB_ID\"}}"
remove_event_rule
delete_temp_files
find_modules
run_terragrunt
put_outputs_to_s3
send_sqs_status "{\"message\":{\"status\":\"Job Completed\",\"progress\":$FINAL_PROGRESS,\"jobId\":\"$AWS_BATCH_JOB_ID\"}}"
upload_temp_folder
echo "End of job routine for $AWS_BATCH_JOB_ID"

