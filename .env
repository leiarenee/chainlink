# WARNING - Do not Change this file, instead make your custom changes in ".dev.env" file in your dev environment.
# These are used as default values in run-pipeline and overwriten by Step Functions.

# PYTHON
PYTHONPATH=src
PYTHONTRACEMALLOC=25

# NPY
NPY_PYTHON_MODULE=lambda_function
NPY_VENV_DIR=venv
NPY_PYTHON_VERSION=3.9.8
NPY_PYTHON_COMMAND=python3.9
NPY_SERVER_MODULE=aws-lambda-rie
NPY_DOCKER_IMAGE_NAME=lab-backend-api
NPY_DOCKER_CLEANUP=true
NPY_DOCKER_PLATFORM=linux/amd64

# WORKSPACE
# WORKSPACE_ID is a unique identifier to differentiate state files. 
WORKSPACE_ID=testing
WORKSPACE_NAME=Testing Environment

# REPOSITORY
VCS_PROVIDER=github
REPO_ACCOUNT=leiarenee
REPO_NAME=sf-infra
REPO_REFERENCE=main
REPO_TYPE=public
USER_NAME=Test User
USER_EMAIL=test@me.com


# SECRETS are fetched from AWS Secrets manager during provisining state of Step functions. 
# You should define them in oveeride.env file in order to run your dev environment.
# use repo access token only if repository is private. 
REPO_ACCESS_TOKEN={"user":"","token":"dummy"}
PIPELINE_AWS_ACCESS={"aws_access_key_id":"dummy","aws_secret_access_key":"dummy"}
TARGET_AWS_ACCESS={"aws_access_key_id":"dummy","aws_secret_access_key":"dummy"}

# VERSIONS
TERRAFORM_VERSION=1.2.7
TERRAGRUNT_VERSION=0.38.7

# TERRAGRUNT
RUN_ALL=true
TG_COMMAND=validate
TG_ARGUMENTS=
# If RUN_MODULE is assigned a value pipeline will execute only specified module
RUN_MODULE=
STACK_FOLDER=live
BUCKET_SUFFIX=dev
INTERACTIVE=false
FORCE_INIT=true
COMPACT_WARNINGS=true
LOG_LEVEL=info

# AWS
TARGET_AWS_REGION=eu-west-1
TARGET_AWS_ACCOUNT_NAME=testing
TARGET_AWS_ACCOUNT_ID=553688522943
TARGET_AWS_PROFILE=leia-testing
PIPELINE_AWS_REGION=eu-west-1
PIPELINE_AWS_PROFILE=leia-pipeline
PIPELINE_AWS_ACCOUNT_ID=377449198785
ECR_AWS_ACCOUNT_ID=377449198785
SQS_AWS_PROFILE=leia-pipeline

# REPLACE Parameters
DOMAIN=testing.dev.leiarenee.io
DNS_ZONE_ID=Z0890541BQO7OVB8F6WL
CERTIFICATE=arn:aws:acm:eu-west-1:377449198785:certificate/431ea958-254b-4f8c-995f-a311559fcce5
CLUSTER=my-testing-k8s

# PIPELINE
RUNNER_MACHINE=local-dev
WORK_FOLDER_NAME=temp-volume
INITIAL_PROGRESS=10
MODULES_FINAL_PROGRESS=90
FINAL_PROGRESS=95
S3_JOBS_BUCKET=sf-pipeline-jobs
EVENTBRIDGE_RULE=SF-Cron-Dev
UPLOAD_WORKFOLDER=always
UPLOAD_JOB_RESOURCES=true
SEND_SQS_MESSAGES=true
UPLOAD_PLAN_FILES=true

# JOB
JOB_DEFINITION=arn:aws:batch:eu-west-1:377449198785:job-definition/tf_pipeline_job_definition
JOB_QUEUE=arn:aws:batch:eu-west-1:377449198785:job-queue/tf-deployment-job-queue

# ROLE
ROLE_EVENTBRIDGE=arn:aws:iam::377449198785:role/service-role/Amazon_EventBridge_Invoke_Batch_Job_Queue

# TTL
TTL_DURATION_MINUTES=10

# LOCAL
LOCAL_APP_REPO=sf-infra
USE_LOCAL_REPO=false

# STATE MACHINE
PIPELINE_STATE_MACHINE_NAME=Pipeline-State-Machine-enabling-drake
PIPELINE_SF_TEMPLATE_FILE=./sf-run/sf-template.json

# SQS
POLL_INTERVAL=5
MAX_SQS_MESSAGES=10

# DOCKER
#DOCKER_RUN_PLATFORM=linux/amd64
#DOCKER_BUILD_PLATFORM=linux/amd64
DOCKER_RUN_PLATFORM=linux/arm64
DOCKER_BUILD_PLATFORM=linux/amd64
DOCKER_FILE_BUILD=Dockerfile
DOCKER_FILE_RUN=Dockerfile.arm64

# BASH
ECHO_COMMANDS=false
