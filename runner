#!/bin/bash
set -e
trap 'catch $? $LINENO' ERR

function catch() {
  echo "Bash Script Error $1 occurred on Line $2"
  bash_error=true
  if [[ $UPLOAD_WORKFOLDER == "always" ]] || [[ $UPLOAD_WORKFOLDER == "onerror" && $tg_err -ne 0 ]]
  then
    set +e
    upload_temp_folder
    set -e
  fi
}

function job_definitions(){
  echo "Pipeline Runner started with arguments \"$@\""
  script_dir=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
  export REPO_ROOT=$script_dir
  if [[ "$RUNNER_MACHINE" != "local-dev" ]] # For local dev environment
  then
    echo $RUNNER_MACHINE ENVIRONMENT VARIABLES
    env
  else
    uuid=$(uuidgen)
    export SF_EXECUTION_NAME=Dev-$(whoami)-$uuid
    export AWS_BATCH_JOB_ID=Process-$$
    set +e;a=$([ ! -z $TERM ] && clear);set -
    echo 
    echo $RUNNER_MACHINE Environment Variables Loaded
    echo
    source $REPO_ROOT/scripts/import-env-vars.sh
    echo
  fi
  export WORK_FOLDER=$REPO_ROOT/$WORK_FOLDER_NAME
  echo Repository folder : \"$REPO_ROOT\"
  echo Work folder : \"$WORK_FOLDER\"
  [ ! -d $WORK_FOLDER ] && mkdir $WORK_FOLDER_NAME
  [ -d $WORK_FOLDER/temp-job ] && rm -rf $WORK_FOLDER/temp-job
  [ ! -d $WORK_FOLDER/temp-job ] && mkdir $WORK_FOLDER_NAME/temp-job
  
  tg_command=${1:-$TG_COMMAND}
  cron_job=$2

  [ -z $SQS_MESSAGE_GROUP_ID ] && export SQS_MESSAGE_GROUP_ID=$AWS_BATCH_JOB_ID

  job_name=$WORKSPACE_ID/$SF_EXECUTION_NAME/$AWS_BATCH_JOB_ID
}

function fetch_from_gitlab(){
  echo
  echo Fetching repository from gitlab
  echo

  # Download repo from gitlab api, project version can be version tag (ex v0.1.1) or sha commit or branch name
  curl --header "PRIVATE-TOKEN: $token" \
    "https://gitlab.com/api/v4/projects/$GITLAB_PROJECT_ID/repository/archive.zip?sha=$GITLAB_PROJECT_VERSION" \
    -o downloaded_repo.zip

  # Unzip the zipped file
  unzip -o downloaded_repo.zip
  rm downloaded_repo.zip

}

function fetch_from_github(){
  echo
  echo Fetching repository from Github, Ref:$REPO_REFERENCE
  echo

  #repo_download_link=https://api.github.com/repos/${REPO_ACCOUNT}/${REPO_NAME}/tarball/${REPO_REFERENCE}
  # redirects here
  repo_download_link=https://codeload.github.com/${REPO_ACCOUNT}/${REPO_NAME}/legacy.tar.gz/refs/heads/${REPO_REFERENCE}

  [[ $REPO_TYPE == "private" ]] && token_header=--header\="Authorization:token $token" 
  
  cd $WORK_FOLDER/temp-job
  wget $token_header --header=Accept:application/vnd.github.v3.raw -O - $repo_download_link | tar xz
  
  # Rename extracted folder name to repo name
  dir_name=$(ls -d $REPO_ACCOUNT-$REPO_NAME*)
  echo Downloaded directory name : $dir_name
  commit=$(echo $dir_name | sed s/$REPO_ACCOUNT-$REPO_NAME-//g)
  echo Commit Hash : $commit
  export COMMIT_HASH=$commit
  # create a clone
  rsync -a $dir_name/* $WORK_FOLDER/temp-job/$WORKSPACE_ID
  cd $REPO_ROOT
   

}

# Fetch Repo
function fetch_repository(){

  # Get secrets from envvars
  user=$(echo $REPO_ACCESS_TOKEN | jq -r .user)
  token=$(echo $REPO_ACCESS_TOKEN | jq -r .token)

  case $VCS_PROVIDER in

    gitlab )
      fetch_from_gitlab
    ;;

    github )
      fetch_from_github
    ;;

    *)
      echo "Unknown REPO_TYPE, Use One of GITHUB/GITLAB"
      exit -1
    ;;

  esac

  echo
}

function copy_local_app_repo(){
  infra_repo=$( cd $REPO_ROOT/../$LOCAL_APP_REPO "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
  echo Copying source files from Local $infra_repo folder 
  rsync -a --exclude-from=$infra_repo/.gitignore $infra_repo/* $WORK_FOLDER/temp-job/$LOCAL_APP_REPO
  # create a clone
  rsync -a $WORK_FOLDER/temp-job/$LOCAL_APP_REPO/* $WORK_FOLDER/temp-job/$WORKSPACE_ID
  # get commit hash
  export COMMIT_HASH=$(git -C $infra_repo log main..$REPO_REFERENCE --format="%h" -n 1)
  echo Commit Hash : $COMMIT_HASH
  echo Repo Head : $(git -C $infra_repo describe --all)
  echo Changed Files : $(git -C $infra_repo status -s | sed s/??//g | tr -d '\n')
  
}

function fetch_app_repo(){
  
  if [[ $USE_LOCAL_REPO == "true" ]]
  then 
    copy_local_app_repo
  else
    fetch_repository
  fi
  
}

function overwrite_terragrunt(){
  # Copy common folders
  rsync -a $REPO_ROOT/terragrunt/* $WORK_FOLDER/temp-job/$WORKSPACE_ID/
  rsync -a $REPO_ROOT/scripts/* $WORK_FOLDER/temp-job/$WORKSPACE_ID/scripts/
}

# Upload Temp Folder
function upload_temp_folder(){
  # UPLOAD_WORKFOLDER never|always|onerror
  # onerror will run on Terragrunt errors and bash errors
  # never disables it - use for local dev env
  # always runs it on every execution

  if [[ $UPLOAD_WORKFOLDER == "always" ]] || [[ $UPLOAD_WORKFOLDER == "onerror" && $tg_err -ne 0 ]] || [[ ! -z $bash_error && $UPLOAD_WORKFOLDER != "never" ]]
  then
    echo Uploading temp folder
    # Find Dublicates and replace with symlinks
    # Thanks to https://github.com/pauldreik/rdfind
    cd $script_dir
    rdfind -makesymlinks true temp-job
    # Compress Work folder
    tar -czf temp-job.tar.gz temp-job
    # Upload to S3
    aws configure set default.s3.max_concurrent_requests 10
    aws configure set default.s3.multipart_threshold 1000MB # Disable concurrency since it doesn't work in docker
    aws configure set default.s3.multipart_chunksize 100MB
    s3_file_name="$job_name/temp-job/$AWS_BATCH_JOB_ID.tar.gz"
    #aws --profile $PIPELINE_AWS_PROFILE s3 cp temp-job.tar.gz s3://$S3_JOBS_BUCKET/$s3_file_name
    aws --profile $PIPELINE_AWS_PROFILE s3api put-object --bucket $S3_JOBS_BUCKET --key $s3_file_name --body temp-job.tar.gz
    rm temp-job.tar.gz
  fi
}

# Fetch Secrets from secret manager
function fetch_secret {
  echo -e "${GREEN}- Feching Secret $1 from secret manager ${NC}";echo
  secret_value=$(aws secretsmanager get-secret-value --secret-id $2)
  echo "Successfully fetched $1";
  export $1=$(echo $secret_value | jq .SecretString | sed s/[\\]//g  | sed s/^\"//g | sed s/\}\"/\}/g )
}

# Configure aws profile
function configure_aws_profile(){
  pipeline_aws_access_key_id=$(echo $PIPELINE_AWS_ACCESS | jq -r .aws_access_key_id)
  pipeline_aws_secret_access_key=$(echo $PIPELINE_AWS_ACCESS | jq -r .aws_secret_access_key)
  
  # Default
  if [ ! $(cat $HOME/.aws/credentials | grep default) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[default]
aws_access_key_id = $pipeline_aws_access_key_id
aws_secret_access_key = $pipeline_aws_secret_access_key
region = $PIPELINE_AWS_REGION
EOF
  fi

  # Infra Structure account
  if [ ! $(cat $HOME/.aws/credentials | grep $PIPELINE_AWS_PROFILE) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[$PIPELINE_AWS_PROFILE]
aws_access_key_id = $pipeline_aws_access_key_id
aws_secret_access_key = $pipeline_aws_secret_access_key
region = $PIPELINE_AWS_REGION
EOF
  fi

  # Target account
  aws_access_key_id=$(echo $TARGET_AWS_ACCESS | jq -r .aws_access_key_id)
  aws_secret_access_key=$(echo $TARGET_AWS_ACCESS | jq -r .aws_secret_access_key)
  if [ ! $(cat $HOME/.aws/credentials | grep $TARGET_AWS_PROFILE) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[$TARGET_AWS_PROFILE]
aws_access_key_id = $aws_access_key_id
aws_secret_access_key = $aws_secret_access_key
region = $TARGET_AWS_REGION
EOF
  fi
}


function find_modules(){
  echo Getting Modules List...
  #tg_modules=$(python3 -m runtask json-modules)
  tg_groups=$(python3 -m runtask json-groups)
  export TG_MODULES_LIST=$(echo $tg_groups | jq 'values[][]' | jq . --slurp -c)
  echo "Terragrunt will run the modules in the following order."
  echo $tg_groups | jq .
  export TG_MODULES_COMPLETED=[]
  export TG_MODULES_COUNT=$(echo $TG_MODULES_LIST | jq length)
  echo
  echo Total number of modules to be processed: $TG_MODULES_COUNT
  echo
  sleep 1
  
}

function test_progress(){
  export TG_PARRENT_DIR=temp-job
  len=$(($TG_MODULES_COUNT - 1))
  for i in $(seq 0 0)
    do
      source terragrunt/scripts/job_complete.sh $STACK_FOLDER/$(echo $TG_MODULES_LIST | jq -r --arg i $i '.[$i|tonumber]')
    done
}

# Run Terragrunt
function run_terragrunt(){

  # Prepare command line arguments

  if [[ $RUN_ALL == "true" ]] 
  then
    tg_run_all=run-all
    working_dir=$WORK_FOLDER/temp-job/$WORKSPACE_ID/$STACK_FOLDER
  else
    working_dir=$WORK_FOLDER/temp-job/$WORKSPACE_ID/$STACK_FOLDER/$RUN_MODULE
  fi

  if [[ $INTERACTIVE != true ]]
  then
    tg_non_interactive=--terragrunt-non-interactive
  fi


  # Escape / Character
  escaped_stack_path=$(echo "$WORK_FOLDER/temp-job" | sed s/\\//\\\\\\//g)
  
  # Prepare command line arguments for interactive or non-interactive usage
  if [[ $INTERACTIVE == "true" ]]
  then
    stderr_output=1
    log_level=info
  else
    export TG_DISABLE_CONFIRM=true
    stderr_output=/dev/null
    log_level=debug
    no_color=-no-color
  fi
  
  set +e

  # Run terragrunt
    terragrunt $tg_run_all $tg_command \
      --terragrunt-debug --terragrunt-working-dir $working_dir  \
      $tg_non_interactive --terragrunt-log-level $log_level $no_color \
      $TG_ARGUMENTS \
      > >(tee $WORK_FOLDER/temp-job/stdout.log) \
      2> >(tee $WORK_FOLDER/temp-job/stderr.log >&$stderr_output) 

  # Get Exit Code
  export tg_err=$?
  [ $tg_err -ne 0 ] && echo Terragrunt Exitcode: $tg_err

  # Prepare a consise error log
  cat $WORK_FOLDER/temp-job/stderr.log | grep -vE 'level=debug|locals|msg=run_cmd|msg=Detected|msg=Included|msg=\[Partial\]|msg=Executing|msg=WARN|msg=Setting|^$|msg=Generated|msg=Downloading|msg=The|msg=Are|msg=Reading|msg=Copying|msg=Debug|must wait|msg=Variables|msg=Dependency|msg=[0-9] error occurred|Cannot process|=>|msg=Stack|msg=Unable|errors occurred|sensitive|BEGIN RSA|\[0m|You may now|any changes|should now|If you ever|If you forget|- Reusing previous| \* exit status|Include this file|Terraform can guarantee|^\t\* |^ prefix' \
    | sed s/$escaped_stack_path//g > $WORK_FOLDER/temp-job/stderr_filtered.log
  
  # If error occurs write filtered error into terminal (stdout)
  if [ $tg_err -ne 0 ]
  then
  cat $WORK_FOLDER/temp-job/stderr_filtered.log | grep -E 'error|Error'
  exit $tg_err
  fi

  set -e

}

# Put outputs to S3
function put_outputs_to_s3(){
  if [[ $tg_command == "apply" ]] && [[ $RUN_ALL == "true" ]] && [ $tg_err -eq 0 ] 
  then
    s3_parent_key="$S3_JOBS_BUCKET/$job_name"
    cd $WORK_FOLDER/temp-job/$WORKSPACE_ID/$STACK_FOLDER/job-resources
    TG_DISABLE_CONFIRM=true terragrunt output job_resources > outputs.txt 

    if [[ $UPLOAD_JOB_RESOURCES == "true" ]]
    then
      echo "Outputs for Job $job_name as text"
      cat outputs.txt
      aws --profile $PIPELINE_AWS_PROFILE s3 cp outputs.txt s3://$s3_parent_key/job-resources/outputs.txt
    fi

    echo "Outputs for Job $job_name as json"
    cat outputs.txt | jq -r '.|fromjson' > outputs.json
    cat outputs.json | jq .

    if [[ $UPLOAD_JOB_RESOURCES == "true" ]]
    then
      result=$(aws --profile $PIPELINE_AWS_PROFILE s3 cp outputs.json s3://$s3_parent_key/job-resources/outputs.json)
    fi
  fi
}

function delete_temp_files(){
  rm -f *.log
}

function createSqsQueue(){ 
  if [[ $SEND_SQS_MESSAGES == "true" ]]
  then

    if [ -z $cron_job ]
    then
      sqs_queue_name=$SF_EXECUTION_NAME
      message_retention_period=600
    else
      sqs_queue_name=cron-job
      message_retention_period=600000
    fi

    if [ -z $SQS_QUEUE_URL ] || [[ $SQS_QUEUE_URL == "_" ]]
    then
      export SQS_QUEUE_URL=$(aws --profile $SQS_AWS_PROFILE sqs create-queue --queue-name $sqs_queue_name.fifo --attributes "{\"FifoQueue\": \"true\", \"ContentBasedDeduplication\": \"true\",\"MessageRetentionPeriod\":\"$message_retention_period\"}" | jq -r .QueueUrl)
    fi
    
    if [ -z "$SQS_QUEUE_URL" ]
    then
      echo "SQS_QUEUE_URL could not be evaluted."
      exit 1
    fi

    echo "SQS Messaging endpoind for the Job is $SQS_QUEUE_URL"
  fi
}

function send_sqs_status(){
  # send job start sqs message
  if [[ $SEND_SQS_MESSAGES == "true" ]]
  then
    echo "sending SQS message : $sqs_message"
    result=(aws --profile $SQS_AWS_PROFILE sqs send-message --queue-url "$SQS_QUEUE_URL" --message-group-id "$SQS_MESSAGE_GROUP_ID" --message-body "$1")
  fi
}

function remove_event_rule(){
  set +e
  if [[ $tg_command == 'destroy' ]]
  then
    python3 -m runtask delete-event --name $EVENTBRIDGE_RULE
  fi
  set -e
}

function send_progress(){
  sqs_message="{\"message\":{\"status\":\"$1\",\"progress\":$2,\"jobId\":\"$AWS_BATCH_JOB_ID\"}}"
  send_sqs_status $sqs_message
}


# Main Routine
job_definitions $@
echo $job_name Job Started
fetch_app_repo
overwrite_terragrunt
configure_aws_profile
if [[ $1 == "bash" ]];then /bin/bash;exit $?;fi 
createSqsQueue
send_progress "Batch_Job_Started" $INITIAL_PROGRESS
remove_event_rule
delete_temp_files
find_modules
run_terragrunt
put_outputs_to_s3
send_progress "Batch_Job_Finished" $FINAL_PROGRESS
upload_temp_folder
echo "End of job routine for $job_name";echo

